{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning - Neural Networks Basics\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand basic concepts of neural networks\n",
    "- Learn about perceptrons and their components\n",
    "- Implement basic logical operations using perceptrons\n",
    "- Discover limitations of single-layer networks through the XOR problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Deep Learning?\n",
    "\n",
    "Deep learning is a subset of machine learning that uses artificial neural networks inspired by the human brain. The 'deep' refers to the multiple layers in these networks.\n",
    "\n",
    "A neural network consists of:\n",
    "1. Input layer\n",
    "2. Hidden layer(s)\n",
    "3. Output layer\n",
    "\n",
    "Each layer contains nodes (neurons) that process information and pass it to the next layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The McCulloch-Pitts Neuron\n",
    "\n",
    "The basic building block of neural networks is the McCulloch-Pitts neuron, which has three components:\n",
    "\n",
    "1. Weights (w₁, w₂, ..., wₙ) corresponding to synapses\n",
    "2. An adder for summing input signals\n",
    "3. An activation function for determining neuron firing\n",
    "\n",
    "The mathematical representation is:\n",
    "h = Σᵢ wᵢxᵢ\n",
    "\n",
    "where h is the weighted sum and wᵢ are the weights for inputs xᵢ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Logical Operations\n",
    "\n",
    "We'll start by implementing basic logical operations (AND, OR) using a perceptron. These are fundamental building blocks for more complex operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create truth tables for logical operations\n",
    "AND = pd.DataFrame({'x1': (0,0,1,1), 'x2': (0,1,0,1), 'y': (0,0,0,1)})\n",
    "print(\"AND Truth Table:\")\n",
    "display(AND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define our activation function\n",
    "def g(inputs, weights):\n",
    "    \"\"\"Simple threshold activation function\"\"\"\n",
    "    return np.where(np.dot(inputs, weights) > 0, 1, 0)\n",
    "\n",
    "# Define training function\n",
    "def train(inputs, targets, weights, eta, n_iterations):\n",
    "    \"\"\"Train the perceptron\n",
    "    \n",
    "    Parameters:\n",
    "    inputs: input data\n",
    "    targets: target values\n",
    "    weights: initial weights\n",
    "    eta: learning rate\n",
    "    n_iterations: number of training iterations\n",
    "    \"\"\"\n",
    "    # Add bias input\n",
    "    inputs = np.c_[inputs, -np.ones((len(inputs), 1))]\n",
    "    \n",
    "    for n in range(n_iterations):\n",
    "        activations = g(inputs, weights)\n",
    "        weights -= eta * np.dot(np.transpose(inputs), activations - targets)\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize weights and train for AND function\n",
    "w = np.random.randn(3) * 1e-4  # Small random initial weights\n",
    "inputs = AND[['x1','x2']]\n",
    "target = AND['y']\n",
    "\n",
    "# Train the perceptron\n",
    "w = train(inputs, target, w, 0.25, 10)\n",
    "\n",
    "# Test the perceptron\n",
    "print(\"\\nTesting AND function:\")\n",
    "result = g(np.c_[inputs, -np.ones((len(inputs), 1))], w)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment: The XOR Problem\n",
    "\n",
    "Now it's your turn! The XOR (exclusive OR) function returns 1 when inputs are different, and 0 when they are the same.\n",
    "\n",
    "Your tasks:\n",
    "\n",
    "1. Create the truth table for XOR\n",
    "2. Try to train a perceptron to learn XOR\n",
    "3. Analyze the results\n",
    "4. Explain why the perceptron succeeds or fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Your code here\n",
    "# Create XOR truth table\n",
    "XOR = pd.DataFrame({'x1': (0,0,1,1), 'x2': (0,1,0,1), 'y': (0,1,1,0)})\n",
    "print(\"XOR Truth Table:\")\n",
    "display(XOR)\n",
    "\n",
    "# Train a perceptron for XOR\n",
    "# Your implementation here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions to Answer:\n",
    "\n",
    "1. What happens when you try to train the perceptron on XOR?\n",
    "2. Why does this happen? (Hint: Think about linear separability)\n",
    "3. How could you modify the network to successfully implement XOR?\n",
    "4. What does this tell us about the limitations of single-layer perceptrons?\n",
    "\n",
    "Write your answers and analysis below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Analysis\n",
    "[Write your analysis here]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}