{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Architecture Experiments with Wine Dataset\n",
    "\n",
    "In this assignment, you will experiment with different neural network architectures using the Wine dataset from sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Data Preparation (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the wine dataset\n",
    "wine = load_wine()\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "# TODO: Split the data into training (80%) and testing (20%) sets\n",
    "# YOUR CODE HERE\n",
    "X_train, X_test, y_train, y_test = \n",
    "\n",
    "# TODO: Scale the features using StandardScaler\n",
    "# YOUR CODE HERE\n",
    "scaler = \n",
    "X_train_scaled = \n",
    "X_test_scaled = \n",
    "\n",
    "# TODO: Convert labels to one-hot encoding\n",
    "# YOUR CODE HERE\n",
    "y_train_cat = \n",
    "y_test_cat = \n",
    "\n",
    "print(\"Training set shape:\", X_train_scaled.shape)\n",
    "print(\"Test set shape:\", X_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Neural Network Architecture Experiments (60 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_a():\n",
    "    \"\"\"Create Model A: Single Hidden Layer\"\"\"\n",
    "    model = Sequential([\n",
    "        Dense(4, activation='relu', input_shape=(13,)),\n",
    "        Dense(3, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def create_model_b():\n",
    "    \"\"\"Create Model B: Two Hidden Layers\"\"\"\n",
    "    model = Sequential([\n",
    "        Dense(8, activation='relu', input_shape=(13,)),\n",
    "        Dense(4, activation='relu'),\n",
    "        Dense(3, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def create_model_c():\n",
    "    \"\"\"Create Model C: Wide Single Layer\"\"\"\n",
    "    model = Sequential([\n",
    "        Dense(16, activation='relu', input_shape=(13,)),\n",
    "        Dense(3, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def train_and_evaluate(model, lr, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Train and evaluate a model with given learning rate\"\"\"\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(lr),\n",
    "                 loss='categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "    \n",
    "    history = model.fit(X_train, y_train,\n",
    "                       epochs=50,\n",
    "                       batch_size=32,\n",
    "                       validation_split=0.2,\n",
    "                       verbose=1)\n",
    "    \n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "    return history, test_loss, test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment with Model A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train Model A with different learning rates\n",
    "learning_rates = [0.1, 0.01, 0.001]\n",
    "results_a = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    model_a = create_model_a()\n",
    "    history, loss, acc = train_and_evaluate(model_a, lr, X_train_scaled, y_train_cat, X_test_scaled, y_test_cat)\n",
    "    results_a.append((lr, history, loss, acc))\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title(f'Model A - Accuracy (lr={lr})')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title(f'Model A - Loss (lr={lr})')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment with Model B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train Model B with different learning rates\n",
    "# YOUR CODE HERE (similar to Model A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment with Model C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train Model C with different learning rates\n",
    "# YOUR CODE HERE (similar to Model A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Hyperparameter Comparison Table (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comparison table\n",
    "results = {\n",
    "    'Model': ['A', 'A', 'A', 'B', 'B', 'B', 'C', 'C', 'C'],\n",
    "    'Hidden Layers': [1, 1, 1, 2, 2, 2, 1, 1, 1],\n",
    "    'Neurons': ['[4]', '[4]', '[4]', '[8,4]', '[8,4]', '[8,4]', '[16]', '[16]', '[16]'],\n",
    "    'Learning Rate': [0.1, 0.01, 0.001, 0.1, 0.01, 0.001, 0.1, 0.01, 0.001],\n",
    "    'Test Accuracy': [# TODO: Fill in your results]\n",
    "}\n",
    "\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Analysis Questions (10 points)\n",
    "\n",
    "Answer the following questions based on your experiments:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Which architecture performed best? Why?\n",
    "\n",
    "YOUR ANSWER HERE\n",
    "\n",
    "2. How did learning rate affect training?\n",
    "\n",
    "YOUR ANSWER HERE\n",
    "\n",
    "3. What is the impact of adding more neurons?\n",
    "\n",
    "YOUR ANSWER HERE\n",
    "\n",
    "4. Which activation function would you try next?\n",
    "\n",
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}